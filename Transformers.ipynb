{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50dd7b6b-6622-406b-812d-7ce55dc3e86d",
   "metadata": {},
   "source": [
    "# Transformers, Self-Attention, BERT, GPT"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d4ca5f21-d3da-464b-bf02-5e5c672fc732",
   "metadata": {},
   "source": [
    "1. Why Transformers? (What Problem They Solve)\n",
    "\n",
    "Before Transformers, NLP models used:\n",
    "\n",
    "‚úî RNN\n",
    "‚úî LSTM\n",
    "‚úî GRU"
   ]
  },
  {
   "cell_type": "raw",
   "id": "649dae56-1e61-4ab1-a7a3-162cdb5291b6",
   "metadata": {},
   "source": [
    "Limitations of RNN/LSTM (Why we need Transformers)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "295f65e3-58e9-4c94-a80b-5e6b13664f2c",
   "metadata": {},
   "source": [
    "1. Sequential Processing ‚Üí Slow\n",
    "He ‚Üí reads ‚Üí every ‚Üí word ‚Üí step ‚Üí by ‚Üí step\n",
    "You cannot parallelize this."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d59db6ed-f55f-407a-a500-72a3ae5872aa",
   "metadata": {},
   "source": [
    "2. Long-term dependency problem\n",
    "Even LSTM struggles when the important information is far away:\n",
    "\n",
    "The book that John gave to Mary,\n",
    "who lives in France...\n",
    "...was expensive."
   ]
  },
  {
   "cell_type": "raw",
   "id": "746fdf2c-9868-4f17-bdfb-788ca03dd1ce",
   "metadata": {},
   "source": [
    "Model must connect ‚Äúbook‚Äù ‚Üî ‚Äúwas expensive‚Äù, which are 15‚Äì30 tokens apart."
   ]
  },
  {
   "cell_type": "raw",
   "id": "575cb4bf-61d4-4794-ab56-bf6ad5598ee2",
   "metadata": {},
   "source": [
    "3. Memory bottleneck\n",
    "Hidden state = single vector capturing all past context ‚Üí compresses too much.\n",
    "\n",
    "4. Slow training\n",
    "No GPU-friendly parallel matrix multiplication."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3425be4f-847d-4d0b-a12f-a17f585c55a2",
   "metadata": {},
   "source": [
    "Transformers solve all 4 problems at once\n",
    "üåü Key ideas\n",
    "\n",
    "Self-Attention\n",
    "Layer Normalization + Residuals\n",
    "Multi-Head Attention\n",
    "Fully Parallel Processing\n",
    "This changed NLP forever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f92d7a-b861-44ef-a128-262b2be9d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers torch datasets sentencepiece accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42920dab-10c3-4f49-bb81-2cf5a473cd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForCausalLM\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff39f668-9676-46b8-ae93-8dab68498810",
   "metadata": {},
   "source": [
    "## Self-Attention: The Core Idea"
   ]
  },
  {
   "cell_type": "raw",
   "id": "82607583-a237-42f8-9390-8ee17bdc92c7",
   "metadata": {},
   "source": [
    "This is the entire reason Transformers replaced RNN/LSTM.\n",
    "\n",
    "Self-attention answers:\n",
    "\n",
    "üëâ \"For every word, how much attention should I pay to every other word?\"\n",
    "\n",
    "The cat sat on the mat because it was warm.\n",
    "it ‚Üí refers to ‚Üí the mat\n",
    "\n",
    "A standard RNN cannot do this well."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d455e064-9fc8-41a2-a9d4-26310d495b75",
   "metadata": {},
   "source": [
    "üî• Self-Attention calculates relationships directly\n",
    "\n",
    "For each word we compute:\n",
    "\n",
    "Q = Query\n",
    "K = Key\n",
    "V = Value\n",
    "\n",
    "Using learned matrices:\n",
    "\n",
    "Q = XWq  \n",
    "K = XWk  \n",
    "V = XWv\n",
    "\n",
    "Then attention score is:\n",
    "\n",
    "Attention(Q, K, V) = Softmax( Q * K·µÄ / ‚àöd ) * V\n",
    "\n",
    "This tells the model:\n",
    "\n",
    "Which words matter\n",
    "How much they matter\n",
    "How to combine them"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e509b54-07de-4a0a-8a62-2e7a7869f335",
   "metadata": {},
   "source": [
    "üß† Intuition of Q, K, V\n",
    "\n",
    "Query (Q): ‚ÄúWhat are you looking for?‚Äù\n",
    "Key (K): ‚ÄúWhat do I contain?‚Äù\n",
    "Value (V): ‚ÄúWhat information should you take from me?‚Äù\n",
    "Let‚Äôs take the sentence: I want to book a flight to Paris.\n",
    "\n",
    "When predicting ‚ÄúParis,‚Äù the model should attend to ‚Äúflight‚Äù and ‚Äúto‚Äù ‚Äî Q/K/V make this possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b828f83c-551c-4c9d-b767-bddebc127740",
   "metadata": {},
   "source": [
    "### Multi-Head Attention (Why many heads?)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a92d5b48-f106-4a0a-a6a0-d9d954625aab",
   "metadata": {},
   "source": [
    "Instead of a single attention pattern, the model learns many patterns in parallel.\n",
    "\n",
    "Each head focuses on something different:\n",
    "\n",
    "Head 1 ‚Üí subject-verb relationship\n",
    "Head 2 ‚Üí nouns\n",
    "Head 3 ‚Üí objects\n",
    "Head 4 ‚Üí locations\n",
    "\n",
    "This is like having multiple interpreters reading the sentence."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad2033c7-b72f-4e1f-afae-8a8789066da1",
   "metadata": {},
   "source": [
    "Visualize self-attention using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3293d6db-8fee-40b1-bc7c-334a4aaee820",
   "metadata": {},
   "source": [
    "Visualize self-attention using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31e86de-772a-428f-8f14-bd14b57cd6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Dummy sequence of 5 tokens, embedding dim = 4\n",
    "x = torch.randn(5, 4)\n",
    "\n",
    "Wq = torch.randn(4, 4)\n",
    "Wk = torch.randn(4, 4)\n",
    "Wv = torch.randn(4, 4)\n",
    "\n",
    "Q = x @ Wq\n",
    "K = x @ Wk\n",
    "V = x @ Wv\n",
    "\n",
    "scores = (Q @ K.T) / (4 ** 0.5)   # scaled dot-product attention\n",
    "attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "context_vector = attn @ V\n",
    "\n",
    "attn, context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98956a18-c641-49e6-935f-2831cddd58c7",
   "metadata": {},
   "source": [
    "### Transformer Architecture (Encoder/Decoder)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "812f6b91-6522-436e-9a7a-d1885ab20fe9",
   "metadata": {},
   "source": [
    "+-------------------------------+\n",
    "|        Transformer            |\n",
    "+---------------+---------------+\n",
    "|   Encoder     |   Decoder     |\n",
    "+---------------+---------------+"
   ]
  },
  {
   "cell_type": "raw",
   "id": "639e7fba-824c-479d-8db2-ea0b03ec9a94",
   "metadata": {},
   "source": [
    "‚úî Encoder (BERT-like models)\n",
    "\n",
    "Bidirectional attention\n",
    "Reads full context\n",
    "Best for classification, embeddings, semantic tasks\n",
    "\n",
    "‚úî Decoder (GPT-like models)\n",
    "\n",
    "Left-to-right attention\n",
    "Predicts next token\n",
    "Best for generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a1ae79-860e-455c-bdcc-ad06928128ae",
   "metadata": {},
   "source": [
    "BERT Explained (Encoder Only)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b3a3f478-952a-4d08-98f2-2d5f6556efae",
   "metadata": {},
   "source": [
    "‚úî BERT uses Bidirectional attention\n",
    "\n",
    "It reads sentences in both directions: The [MASK] sat on the mat.\n",
    "\n",
    "BERT predicts:\n",
    "‚Äúcat‚Äù\n",
    "\n",
    "BERT pretraining tasks:\n",
    "\n",
    "1. Masked Language Modeling\n",
    "(Hides tokens and predicts them)\n",
    "\n",
    "2. Next Sentence Prediction\n",
    "(Understood relationships between sentences)\n",
    "\n",
    "This makes BERT perfect for:\n",
    "‚úî Classification\n",
    "‚úî NER\n",
    "‚úî Semantic search\n",
    "‚úî Q&A\n",
    "‚úî Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a53835-a2b0-4d2e-8d6a-849f98f26325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "bert = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "bert(\"The capital of France is [MASK].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39b8f23-ee43-45b8-bd92-38de6801113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "bert = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "bert(\"The capital of France is [MASK].\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3bdc7c-cb59-47ff-a28a-866885bf4f40",
   "metadata": {},
   "source": [
    "### GPT Explained (Decoder Only)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0b54587-df0c-436a-8edc-59b0f344fe1a",
   "metadata": {},
   "source": [
    "GPT uses causal attention (no looking ahead):\n",
    "\n",
    "I like to eat ice\n",
    "           ^\n",
    "           predict ‚Äúcream‚Äù"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2bc5b5ba-c249-46f4-b81b-834216edad87",
   "metadata": {},
   "source": [
    "GPT pretraining:\n",
    "\n",
    "‚úî Next-token prediction\n",
    "\n",
    "This makes GPT ideal for:\n",
    "\n",
    "Chatbots\n",
    "Generation\n",
    "Summarization\n",
    "Code generation\n",
    "Agentic AI\n",
    "Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36458f7-c238-4727-aacb-1eccf6d9ccce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "gpt = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "gpt(\"Once upon a time in Bangalore,\")[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba7784f-753a-4e4a-8ad6-5e31f0fc6d77",
   "metadata": {},
   "source": [
    "#### BERT vs GPT (Simple Table)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1085ebdc-de01-492d-99ba-7f531caa904c",
   "metadata": {},
   "source": [
    "| Feature   | BERT           | GPT                   |\n",
    "| --------- | -------------- | --------------------- |\n",
    "| Type      | Encoder        | Decoder               |\n",
    "| Direction | Bidirectional  | Left ‚Üí Right          |\n",
    "| Task      | Understanding  | Generation            |\n",
    "| Training  | Masked LM      | Next-token prediction |\n",
    "| Best for  | Classification | Text Generation       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094d20b6-ccf9-4280-9a36-4bd0793e90d9",
   "metadata": {},
   "source": [
    "#### Self-Attention is Parallel ‚Üí Fast on GPUs"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a9d6828-487a-4a6b-9c67-d576cf915d05",
   "metadata": {},
   "source": [
    "RNN:\n",
    "t1 ‚Üí t2 ‚Üí t3 ‚Üí t4 ‚Üí t5 (sequential)\n",
    "\n",
    "Transformer attention:\n",
    "t1  t2  t3  t4  t5 (all parallel)\n",
    "\n",
    "This is why GPT-4 can be trained on thousands of GPUs."
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d6417c8-02dd-43b1-8e2c-6c069c158084",
   "metadata": {},
   "source": [
    "Positional Encoding ‚Äî Why Needed?\n",
    "\n",
    "Transformers don‚Äôt know order by default.\n",
    "So we add sinusoidal patterns: sin(position), cos(position)\n",
    "\n",
    "This lets model know:\n",
    "token #1\n",
    "token #2\n",
    "token #3\n",
    "token #...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245825e1-9b4b-477b-a5f8-54e6748750e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(seq_len, d_model):\n",
    "    PE = torch.zeros(seq_len, d_model)\n",
    "    pos = torch.arange(0, seq_len).unsqueeze(1).float()  # shape [seq_len, 1]\n",
    "\n",
    "    for i in range(0, d_model, 2):\n",
    "        div_term = 10000 ** (i / d_model)\n",
    "        PE[:, i] = torch.sin(pos.squeeze(1) / div_term)  # shape [seq_len]\n",
    "        PE[:, i+1] = torch.cos(pos.squeeze(1) / div_term)\n",
    "    return PE\n",
    "\n",
    "PE = positional_encoding(50, 16)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.heatmap(PE.numpy(), cmap=\"viridis\")\n",
    "plt.title(\"Positional Encoding Pattern\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83f46d3-94fe-491b-a29e-7f8ad0540f9c",
   "metadata": {},
   "source": [
    "Full Transformer Block Diagram"
   ]
  },
  {
   "cell_type": "raw",
   "id": "73e96bb0-2aae-4ee3-b107-f9211931c8c4",
   "metadata": {},
   "source": [
    "Input Embedding + Position\n",
    "        ‚îÇ\n",
    "        ‚ñº\n",
    "+------------------------+\n",
    "|   Multi-Head Attention |\n",
    "+------------------------+\n",
    "        ‚îÇ\n",
    "   Add & Norm\n",
    "        ‚îÇ\n",
    "        ‚ñº\n",
    "+------------------------+\n",
    "|  Feed Forward (MLP)    |\n",
    "+------------------------+\n",
    "        ‚îÇ\n",
    "   Add & Norm\n",
    "        ‚ñº\n",
    "   Output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542fa8d0-a02d-41cd-9aaf-24b45bfa3e8d",
   "metadata": {},
   "source": [
    "Mini Transformer Attention Visualization (HF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bc434f-567c-4a0b-96de-b37095172869",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "inputs = tokenizer(\"Hello I am learning transformers!\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "attn = outputs.attentions  \n",
    "len(attn), attn[0].shape   # (layers, batch, heads, tokens, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caf0389-2d7b-4ed3-bcae-ef2d8bd0112e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f444c2e0-8e96-46ae-a364-8d1a4f7212da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "fill(\"The capital of France is [MASK].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3355bf11-d69e-45dc-9f68-46ea9337e549",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "clf(\"I really love deep learning. It is amazing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020f9dde-5034-49b6-bfc1-3b1a8c2812cf",
   "metadata": {},
   "source": [
    "BERT Sentence Embeddings (Used in RAG, LangChain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc0cd63-19c1-4ace-9722-90568a2bdbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "text = \"Transformers changed deep learning forever.\"\n",
    "tokens = tok(text, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(**tokens).last_hidden_state\n",
    "    embedding = output.mean(dim=1)\n",
    "\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e931cd-f306-482f-afad-d860d8fd539d",
   "metadata": {},
   "source": [
    "BERT Attention Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b5f964-cf3a-49e3-8052-cfc04efacaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n",
    "tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "inputs = tok(\"Attention is all you need\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "attn = outputs.attentions[0][0]   # 1st layer, 1st batch = (heads, tokens, tokens)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.heatmap(attn.mean(0).detach().numpy(), cmap=\"Reds\")\n",
    "plt.title(\"BERT Attention Heatmap (Layer 1, Avg Heads)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76dbe8c-c652-40af-8493-efe3e905f7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = pipeline(\"text-generation\", model=\"gpt2\", max_length=60)\n",
    "gen(\"Once upon a time in Bangalore\")[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b056e2-96b2-429a-996d-0725781b2157",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- BERT (Understanding) ---\")\n",
    "print(fill(\"The weather today is very [MASK].\"))\n",
    "\n",
    "print(\"\\n--- GPT (Generation) ---\")\n",
    "print(gen(\"The weather today is very\")[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "653adc16-e66b-45bc-b99d-b7df5465fc8c",
   "metadata": {},
   "source": [
    "üß© When to Use What (Final Summary)\n",
    "üîµ Use BERT for understanding\n",
    "\n",
    "Classification\n",
    "Sentiment\n",
    "NER\n",
    "Semantic Search\n",
    "RAG embeddings\n",
    "Topic detection\n",
    "Document similarity\n",
    "\n",
    "üî¥ Use GPT for generating\n",
    "\n",
    "Chatbots\n",
    "Summaries\n",
    "Story writing\n",
    "Code generation\n",
    "Reasoning agents (LangChain, MCP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dcdd28-c88b-4c29-94c5-4223e4952869",
   "metadata": {},
   "source": [
    "How Transformers Power Agentic AI"
   ]
  },
  {
   "cell_type": "raw",
   "id": "28569651-db45-461b-b59a-72f6b143b3e3",
   "metadata": {},
   "source": [
    "Transformers ‚Üí\n",
    "\n",
    "Give embeddings\n",
    "Provide reasoning\n",
    "Support function calling\n",
    "Enable tool use\n",
    "Can be fine-tuned to behave like agents\n",
    "\n",
    "BERT ‚Üí\n",
    "Used in vector DBs, similarity, retrieval.\n",
    "\n",
    "GPT ‚Üí\n",
    "Used for reasoning, planning, routing, tool orchestration.\n",
    "Without Transformers, agentic systems wouldn‚Äôt be possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db883b4-fe0e-4205-9c35-df789c2a0e17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
