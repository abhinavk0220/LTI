{
 "cells": [
  {
   "cell_type": "raw",
   "id": "1db9076e-a1b0-4991-acb6-cb26a688e2e7",
   "metadata": {},
   "source": [
    "### 1. The Problem: Full Fine-Tuning is Insane\n",
    "\n",
    "| Model        | Parameters | VRAM to full fine-tune (FP16) | Can you run it on free Colab? |\n",
    "|--------------|------------|-------------------------------|-------------------------------|\n",
    "| BERT-base    | 110 M      | ~8–10 GB                      | Yes (barely)                  |\n",
    "| LLaMA-7B     | 7 B        | ~60+ GB                       | No                            |\n",
    "| LLaMA-3-8B   | 8 B        | ~70 GB                        | No                            |\n",
    "| Mixtral 8x7B | 46 B       | ~400 GB                       | Only on supercomputers       |\n",
    "\n",
    "→ Full fine-tuning = you update **all** weights → you need to store gradients + optimizer states for every single parameter → memory explodes."
   ]
  },
  {
   "cell_type": "raw",
   "id": "39e80e4e-493a-49ff-a970-999f52acf0b3",
   "metadata": {},
   "source": [
    "### 2. LoRA = The 0.1% Miracle (2019 → 2021 paper, exploded in 2023)\n",
    "\n",
    "**Idea**: During fine-tuning, most weights in a transformer barely change. The useful adaptation happens in a very low-dimensional subspace.\n",
    "\n",
    "**What LoRA does** (super simple):\n",
    "\n",
    "Instead of updating the huge weight matrix W (e.g., 4096×4096 = 16 million parameters),  \n",
    "LoRA **freezes W** and injects two tiny matrices A and B:\n",
    "\n",
    "```\n",
    "Updated weight = W (frozen) + ΔW\n",
    "               = W + B @ A\n",
    "```\n",
    "\n",
    "Where:\n",
    "- B is d × r  \n",
    "- A is r × k  \n",
    "- r = rank (usually 8, 16, 32, 64) → super small!\n",
    "\n",
    "Example for LLaMA-7B:\n",
    "- Original attention weight: 4096 × 4096 = 16 M params\n",
    "- LoRA (r=16): B = 4096×16, A = 16×4096 → only 4096×16×2 ≈ **130k** parameters per layer!\n",
    "- Apply to query + value layers (8–32 layers) → total trainable parameters ≈ **4–20 million** instead of 7 billion → **0.1%–0.3%**\n",
    "\n",
    "That’s why you see:\n",
    "```python\n",
    "model.print_trainable_parameters()\n",
    "# \"trainable params: 4,194,073 || all params: 7,241,757,696 || trainable%: 0.058%\"\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6b07bd3e-6458-47f3-b9b3-23c7364f8e22",
   "metadata": {},
   "source": [
    "### 3. QLoRA = LoRA on Steroids (May 2023 paper → made 65B models fine-tunable on 1 GPU)\n",
    "\n",
    "QLoRA adds **3 more tricks** on top of LoRA:\n",
    "\n",
    "| Trick                    | What it does                                     | Memory saved |\n",
    "|--------------------------|--------------------------------------------------|--------------|\n",
    "| 4-bit quantization       | Stores original 7B weights in 4-bit (~0.5 GB/1B) | ~75%         |\n",
    "| Double quantization      | Quantizes the quantization constants             | extra ~10%   |\n",
    "| Paged optimizer          | Moves optimizer states to CPU when not needed    | huge for long training |\n",
    "\n",
    "Result:\n",
    "- LLaMA-65B full fine-tune → needs ~800+ GB\n",
    "- QLoRA 65B → runs on a single 24-GB RTX 4090 (or 2×24 GB)\n",
    "- QLoRA 33B → runs on free Colab Pro+ (A100 40GB) or even T4 16GB with tricks\n",
    "\n",
    "### 4. When to Use What? (2025 Decision Tree)\n",
    "\n",
    "| Your Setup / Goal                              | Use This                     | Why                                                                 |\n",
    "|------------------------------------------------|------------------------------|---------------------------------------------------------------------|\n",
    "| Small model (<3B), lots of GPU memory         | Full fine-tune               | Slightly better quality, simple                                      |\n",
    "| 7B–13B model, want best quality                | LoRA (r=64) + FP16/BF16      | 99% of full fine-tune quality, 10–20× less memory                   |\n",
    "| 7B–70B model, only 1 consumer GPU (24GB)      | QLoRA 4-bit                  | Only way to fit it                                                  |\n",
    "| Free Google Colab / Kaggle (T4 16GB)           | QLoRA 4-bit + r=16–32        | Fits 7B easily, 13B with gradient checkpointing                    |\n",
    "| You want to merge back into base model         | LoRA (not QLoRA)             | 4-bit weights can't be perfectly merged                             |\n",
    "| You care about inference speed                 | LoRA → merge → GGUF (llama.cpp) | QLoRA models are slower unless de-quantized                        |\n",
    "| You are doing research / need max performance  | Full fine-tune or high-rank LoRA | Squeeze out last 1–2% accuracy       "
   ]
  },
  {
   "cell_type": "raw",
   "id": "8ce4e552-d9cd-40cf-95bd-9656415c423b",
   "metadata": {},
   "source": [
    "### Real-World Numbers (2025)\n",
    "\n",
    "| Model          | Method     | GPU needed       | Trainable params | Final quality vs full |\n",
    "|----------------|------------|------------------|------------------|-----------------------|\n",
    "| LLaMA-3-8B     | Full       | 8×A100 (320GB)   | 8B               | 100%                  |\n",
    "| LLaMA-3-8B     | LoRA r=64  | 1×A100 (40GB)    | ~20M             | 99.5%                 |\n",
    "| LLaMA-3-8B     | QLoRA 4bit | 1×RTX 4090 (24GB)| ~20M             | 99.0–99.3%            |\n",
    "| LLaMA-3-70B    | QLoRA      | 1×A100 (80GB)    | ~100M            | 98–99%                |\n",
    "\n",
    "### Quick Code: LoRA vs QLoRA in 2025\n",
    "\n",
    "```python\n",
    "# Just LoRA (high quality)\n",
    "from peft import LoraConfig, get_peft_model\n",
    "config = LoraConfig(r=64, lora_alpha=128, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05)\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "# QLoRA (fits anywhere)\n",
    "from transformers import BitsAndBytesConfig\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(..., quantization_config=quant_config, device_map=\"auto\")\n",
    "# then apply LoRA on top\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "id": "672cb9a0-2b99-4de7-b9c0-fe7ecb294d93",
   "metadata": {},
   "source": [
    "### Summary Table\n",
    "\n",
    "| Technique | Trainable Params | GPU Memory | Quality | Use When |\n",
    "|---------|------------------|------------|---------|----------|\n",
    "| Full fine-tune | 100% | 10–100× more | 100% | Small models + big GPUs |\n",
    "| LoRA | 0.1–1% | 5–20 GB | 99%+ | Best quality on decent GPU |\n",
    "| QLoRA | 0.1–1% | 10–40 GB for 70B | 98–99% | You have small GPU or want 33B+ models |\n",
    "\n",
    "**2025 reality**:  \n",
    "99% of people doing \"fine-tuning\" today are actually running **QLoRA** and calling it fine-tuning — and they get results almost as good as Meta/OpenAI!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
