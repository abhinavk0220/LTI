{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a323dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from tf_keras import models, layers\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eb370a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_purchase = pd.read_csv(\"User_product_purchase_details_p2.csv\")\n",
    "df_user = pd.read_csv(\"user_demographics.csv\")\n",
    "\n",
    "df = pd.merge(df_purchase, df_user, on=\"User_ID\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42ef35d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape after merge: (550068, 12)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset shape after merge:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9b314f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   User_ID Product_ID City_Category Stay_In_Current_City_Years  \\\n",
      "0  1000001  P00069042             A                          2   \n",
      "1  1000001  P00248942             A                          2   \n",
      "2  1000001  P00087842             A                          2   \n",
      "3  1000001  P00085442             A                          2   \n",
      "4  1000002  P00285442             C                         4+   \n",
      "\n",
      "   Marital_Status  Product_Category_1  Product_Category_2  Product_Category_3  \\\n",
      "0               0                   3                 NaN                 NaN   \n",
      "1               0                   1                 6.0                14.0   \n",
      "2               0                  12                 NaN                 NaN   \n",
      "3               0                  12                14.0                 NaN   \n",
      "4               0                   8                 NaN                 NaN   \n",
      "\n",
      "   Purchase Gender   Age  Occupation  \n",
      "0      8370      F  0-17          10  \n",
      "1     15200      F  0-17          10  \n",
      "2      1422      F  0-17          10  \n",
      "3      1057      F  0-17          10  \n",
      "4      7969      M   55+          16  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b92e2ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target distribution:\n",
      "High_Value_Purchase\n",
      "0    360529\n",
      "1    189539\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df[\"High_Value_Purchase\"] = (df[\"Purchase\"] >= 10000).astype(int)\n",
    "\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(df[\"High_Value_Purchase\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62e4ef32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values before filling:\n",
      "City_Category                      0\n",
      "Stay_In_Current_City_Years         0\n",
      "Marital_Status                     0\n",
      "Product_Category_1                 0\n",
      "Product_Category_2            173638\n",
      "Product_Category_3            383247\n",
      "Purchase                           0\n",
      "Gender                             0\n",
      "Age                                0\n",
      "Occupation                         0\n",
      "High_Value_Purchase                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = df.drop([\"Product_ID\", \"User_ID\"], axis=1)\n",
    "\n",
    "# Handle missing values\n",
    "print(\"\\nMissing values before filling:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "df = df.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b004fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables using one-hot encoding\n",
    "categorical_cols = ['Gender', 'Age', 'City_Category', 'Stay_In_Current_City_Years', 'Marital_Status']\n",
    "\n",
    "# Check which categorical columns exist in the dataframe\n",
    "existing_categorical_cols = [col for col in categorical_cols if col in df.columns]\n",
    "\n",
    "df = pd.get_dummies(df, columns=existing_categorical_cols, drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43dc7439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset shape after encoding: (550068, 20)\n",
      "\n",
      "Column names after encoding:\n",
      "['Product_Category_1', 'Product_Category_2', 'Product_Category_3', 'Purchase', 'Occupation', 'High_Value_Purchase', 'Gender_M', 'Age_18-25', 'Age_26-35', 'Age_36-45', 'Age_46-50', 'Age_51-55', 'Age_55+', 'City_Category_B', 'City_Category_C', 'Stay_In_Current_City_Years_1', 'Stay_In_Current_City_Years_2', 'Stay_In_Current_City_Years_3', 'Stay_In_Current_City_Years_4+', 'Marital_Status_1']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDataset shape after encoding:\", df.shape)\n",
    "print(\"\\nColumn names after encoding:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b30f53a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df.drop([\"High_Value_Purchase\", \"Purchase\"], axis=1)\n",
    "y = df[\"High_Value_Purchase\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8d641fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split (80-20)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0a3f7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set size: 440054\n",
      "Test set size: 110014\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nTraining set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa7cd730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numerical features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63144212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "LOGISTIC REGRESSION MODEL\n",
      "==================================================\n",
      "\n",
      "LR Accuracy: 0.7661752140636646\n",
      "\n",
      "Confusion Matrix:\n",
      "[[64702  7404]\n",
      " [18320 19588]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.90      0.83     72106\n",
      "           1       0.73      0.52      0.60     37908\n",
      "\n",
      "    accuracy                           0.77    110014\n",
      "   macro avg       0.75      0.71      0.72    110014\n",
      "weighted avg       0.76      0.77      0.75    110014\n",
      "\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "               feature  importance\n",
      "0   Product_Category_1    0.845970\n",
      "2   Product_Category_3    0.430037\n",
      "4             Gender_M    0.110769\n",
      "12     City_Category_C    0.109819\n",
      "7            Age_36-45    0.076448\n",
      "6            Age_26-35    0.072334\n",
      "9            Age_51-55    0.064008\n",
      "8            Age_46-50    0.043494\n",
      "11     City_Category_B    0.036939\n",
      "5            Age_18-25    0.036241\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LOGISTIC REGRESSION MODEL\")\n",
    "print(\"=\"*50)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "log = LogisticRegression(max_iter=2000, random_state=42)\n",
    "log.fit(X_train_scaled, y_train)\n",
    "\n",
    "pred_lr = log.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\nLR Accuracy:\", accuracy_score(y_test, pred_lr))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, pred_lr))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, pred_lr))\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': np.abs(log.coef_[0])\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f92acebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "MLP NEURAL NETWORK MODEL\n",
      "==================================================\n",
      "WARNING:tensorflow:From c:\\Users\\User\\Documents\\day 5\\ollama-env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\User\\Documents\\day 5\\ollama-env\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\User\\Documents\\day 5\\ollama-env\\Lib\\site-packages\\tf_keras\\src\\optimizers\\__init__.py:317: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "\n",
      "Model Architecture:\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                1216      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3329 (13.00 KB)\n",
      "Trainable params: 3329 (13.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "\n",
      "Training MLP...\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:From c:\\Users\\User\\Documents\\day 5\\ollama-env\\Lib\\site-packages\\tf_keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\User\\Documents\\day 5\\ollama-env\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "11002/11002 [==============================] - 11s 918us/step - loss: 0.3800 - accuracy: 0.8366 - val_loss: 0.3054 - val_accuracy: 0.8780\n",
      "Epoch 2/20\n",
      "11002/11002 [==============================] - 10s 938us/step - loss: 0.2954 - accuracy: 0.8806 - val_loss: 0.2997 - val_accuracy: 0.8753\n",
      "Epoch 3/20\n",
      "11002/11002 [==============================] - 11s 983us/step - loss: 0.2876 - accuracy: 0.8837 - val_loss: 0.2851 - val_accuracy: 0.8889\n",
      "Epoch 4/20\n",
      "11002/11002 [==============================] - 10s 868us/step - loss: 0.2814 - accuracy: 0.8861 - val_loss: 0.2776 - val_accuracy: 0.8873\n",
      "Epoch 5/20\n",
      "11002/11002 [==============================] - 9s 844us/step - loss: 0.2759 - accuracy: 0.8877 - val_loss: 0.2697 - val_accuracy: 0.8898\n",
      "Epoch 6/20\n",
      "11002/11002 [==============================] - 9s 857us/step - loss: 0.2699 - accuracy: 0.8911 - val_loss: 0.2649 - val_accuracy: 0.8959\n",
      "Epoch 7/20\n",
      "11002/11002 [==============================] - 10s 895us/step - loss: 0.2649 - accuracy: 0.8939 - val_loss: 0.2612 - val_accuracy: 0.8939\n",
      "Epoch 8/20\n",
      "11002/11002 [==============================] - 11s 957us/step - loss: 0.2610 - accuracy: 0.8949 - val_loss: 0.2584 - val_accuracy: 0.8962\n",
      "Epoch 9/20\n",
      "11002/11002 [==============================] - 12s 1ms/step - loss: 0.2589 - accuracy: 0.8958 - val_loss: 0.2562 - val_accuracy: 0.8959\n",
      "Epoch 10/20\n",
      "11002/11002 [==============================] - 11s 962us/step - loss: 0.2573 - accuracy: 0.8964 - val_loss: 0.2567 - val_accuracy: 0.8963\n",
      "Epoch 11/20\n",
      "11002/11002 [==============================] - 11s 1ms/step - loss: 0.2565 - accuracy: 0.8971 - val_loss: 0.2543 - val_accuracy: 0.8986\n",
      "Epoch 12/20\n",
      "11002/11002 [==============================] - 11s 1ms/step - loss: 0.2559 - accuracy: 0.8973 - val_loss: 0.2553 - val_accuracy: 0.8987\n",
      "Epoch 13/20\n",
      "11002/11002 [==============================] - 10s 922us/step - loss: 0.2555 - accuracy: 0.8975 - val_loss: 0.2537 - val_accuracy: 0.8985\n",
      "Epoch 14/20\n",
      "11002/11002 [==============================] - 11s 954us/step - loss: 0.2550 - accuracy: 0.8976 - val_loss: 0.2557 - val_accuracy: 0.8977\n",
      "Epoch 15/20\n",
      "11002/11002 [==============================] - 10s 922us/step - loss: 0.2546 - accuracy: 0.8977 - val_loss: 0.2523 - val_accuracy: 0.8988\n",
      "Epoch 16/20\n",
      "11002/11002 [==============================] - 10s 914us/step - loss: 0.2537 - accuracy: 0.8978 - val_loss: 0.2530 - val_accuracy: 0.8995\n",
      "Epoch 17/20\n",
      "11002/11002 [==============================] - 10s 919us/step - loss: 0.2533 - accuracy: 0.8979 - val_loss: 0.2540 - val_accuracy: 0.8984\n",
      "Epoch 18/20\n",
      "11002/11002 [==============================] - 10s 910us/step - loss: 0.2527 - accuracy: 0.8979 - val_loss: 0.2516 - val_accuracy: 0.8989\n",
      "Epoch 19/20\n",
      "11002/11002 [==============================] - 10s 922us/step - loss: 0.2524 - accuracy: 0.8981 - val_loss: 0.2508 - val_accuracy: 0.8993\n",
      "Epoch 20/20\n",
      "11002/11002 [==============================] - 10s 920us/step - loss: 0.2519 - accuracy: 0.8981 - val_loss: 0.2511 - val_accuracy: 0.8998\n",
      "\n",
      "MLP Test Accuracy: 0.8989\n",
      "MLP Test Loss: 0.2513\n",
      "3438/3438 [==============================] - 2s 613us/step\n",
      "\n",
      "Confusion Matrix:\n",
      "[[62461  9645]\n",
      " [ 1481 36427]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.87      0.92     72106\n",
      "           1       0.79      0.96      0.87     37908\n",
      "\n",
      "    accuracy                           0.90    110014\n",
      "   macro avg       0.88      0.91      0.89    110014\n",
      "weighted avg       0.91      0.90      0.90    110014\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MLP NEURAL NETWORK MODEL\")\n",
    "print(\"=\"*50)\n",
    "from tf_keras import models, layers\n",
    "\n",
    "# Build MLP model\n",
    "model = models.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=\"adam\", \n",
    "    loss=\"binary_crossentropy\", \n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Display model architecture\n",
    "print(\"\\nModel Architecture:\")\n",
    "model.summary()\n",
    "\n",
    "# Train model\n",
    "print(\"\\nTraining MLP...\")\n",
    "history = model.fit(\n",
    "    X_train_scaled, \n",
    "    y_train, \n",
    "    epochs=20, \n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "loss, acc = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(f\"\\nMLP Test Accuracy: {acc:.4f}\")\n",
    "print(f\"MLP Test Loss: {loss:.4f}\")\n",
    "\n",
    "# Get predictions for confusion matrix\n",
    "pred_mlp = (model.predict(X_test_scaled) > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, pred_mlp))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, pred_mlp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85981e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "MODEL COMPARISON\n",
      "==================================================\n",
      "\n",
      "Logistic Regression Accuracy: 0.7662\n",
      "MLP Neural Network Accuracy: 0.8989\n",
      "\n",
      "Difference: 0.1327\n",
      "\n",
      "✓ MLP performed better!\n",
      "Reason: Neural networks can capture non-linear relationships\n",
      "between features that logistic regression cannot model.\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "lr_accuracy = accuracy_score(y_test, pred_lr)\n",
    "mlp_accuracy = acc\n",
    "\n",
    "print(f\"\\nLogistic Regression Accuracy: {lr_accuracy:.4f}\")\n",
    "print(f\"MLP Neural Network Accuracy: {mlp_accuracy:.4f}\")\n",
    "print(f\"\\nDifference: {abs(mlp_accuracy - lr_accuracy):.4f}\")\n",
    "\n",
    "if mlp_accuracy > lr_accuracy:\n",
    "    print(\"\\n✓ MLP performed better!\")\n",
    "    print(\"Reason: Neural networks can capture non-linear relationships\")\n",
    "    print(\"between features that logistic regression cannot model.\")\n",
    "elif lr_accuracy > mlp_accuracy:\n",
    "    print(\"\\n✓ Logistic Regression performed better!\")\n",
    "    print(\"Reason: The relationship might be primarily linear, or the\")\n",
    "    print(\"neural network may be overfitting the training data.\")\n",
    "else:\n",
    "    print(\"\\n✓ Both models performed equally!\")\n",
    "    print(\"Reason: The problem might have simple linear patterns that\")\n",
    "    print(\"both models can capture effectively.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e93eb22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
